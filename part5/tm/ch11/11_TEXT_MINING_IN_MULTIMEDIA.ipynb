{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 11. TEXT MINING IN MULTIMEDIA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 바벨피쉬 : NLPpy - 텍스트마이닝 (1)\n",
    "* 김무성"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Contents\n",
    "\n",
    "1. Introduction\n",
    "2. Surrounding Text Mining\n",
    "3. Tag Mining\n",
    "   - 3.1 Tag Ranking\n",
    "   - 3.2 Tag Refinement\n",
    "   - 3.3 Tag Information Enrichment\n",
    "4. Joint Text and Visual Content Mining\n",
    "   - 4.1 Visual Re-ranking\n",
    "5. Cross Text and Visual Content Mining\n",
    "6. Summary and Open Issues\n",
    "   - Joint text and visual content multimedia ranking\n",
    "   - Scalable text mining for large-scale multimedia man- agement\n",
    "   - Multimedia social network mining"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Abstract"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* A multimedia entity does not appear in isolation, but is accompanied by various forms of metadata, such as \n",
    "    - surrounding text, \n",
    "    - user tags, \n",
    "    - ratings, and \n",
    "    - comments etc.\n",
    "* Specifically, the survey focuses on four aspects: \n",
    "    - (a) surrounding text mining; \n",
    "    - (b) tag mining; \n",
    "    - (c) joint text and visual content mining; and \n",
    "    - (d) cross text and visual content mining. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Keywords :\n",
    "* Text Mining, \n",
    "* Multimedia, \n",
    "* Surrounding Text, \n",
    "* Tagging, \n",
    "* Social Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"red\">On the other hand, a multimedia entity does not appear in isola- tion but is accompanied by various forms of textual metadata.</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/cap11.1.png\" width=600 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/cap11.2.png\" width=600 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/cap11.3.png\" width=600 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Surrounding Text Mining"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Developing effective extraction algorithm for the comprehensive analysis of surrounding text has been a very challenging task. \n",
    "    - In many cases, automatically determining which page region is more relevant to the image than the others could be difficult.\n",
    "    - Moreover, how large the region nearby should be considered is still an open question. \n",
    "    - Further, the quality of surrounding texts could be low and inconsistent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The earliest efforts on modeling and analyzing surrounding texts to facilitate <font color=\"red\">multimedia retrieval</font> \n",
    "* AltaVista’s A/V Photo Finder [1]\n",
    "    - The indexing terms are precomputed based on the HTML documents containing the Web images.\n",
    "* WebSeer system [12]\n",
    "    - With a similar approach, the WebSeer system harvests the information for indexing Web images from two different sources:\n",
    "        - the related HTML text\n",
    "            - It extracts keywords from page title, file name, caption, alternative text, image hyperlinks, and body text titles.\n",
    "            - A weight is calculated for each keyword based on its location inside a page.\n",
    "        - and the embedded image itself\n",
    "* PICITION system [40]\n",
    "    - exploit both textual and visual information to index a pictorial database\n",
    "    - Image captions - important cue -> identify faces appearing in a related newspaper photograph\n",
    "    - While the system can be successfully adopted for ac- cessing photographs in newspaper or magazine, it is not straightforward to apply it for Web image retrieval.\n",
    "* WebSeek [39]\n",
    "    - Smith and Chang proposed the WebSeek framework designed to search images from the Web.\n",
    "    - The key idea is to analyze and classify the Web multimedia objects into a predefined taxonomy of categories.\n",
    "    - Thus, an initial search can be performed to explore a catalog associated with the query terms.\n",
    "    - The image attribute (e.g., color histogram for images) is then computed for similarity matching within the category."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Besides its efficacy in image retrieval, surrounding text has been explored for <font color=\"red\">image annotation</font> recently.\n",
    "* predefined semantic concepts [9]\n",
    "    - To achieve better annotation effectiveness, \n",
    "    - a co-training scheme is designed to explore the association between \n",
    "        - the text features computed using corresponding HTML documents and \n",
    "        - visual features extracted from image content.\n",
    "    - Iterative Similarity Propagation\n",
    "        - Observing that the links between the visual content and the surrounding texts can be modeled via Web page analysis\n",
    "        - a novel method called Iterative Simi- larity Propagation is proposed to refine the closeness between the Web images and their annotations [50]\n",
    "* Consequently, accurate clustering is a very crucial technique to facilitate Web multimedia search and many algorithms have recently been proposed based on the analysis of surrounding texts and low level visual features [3][13][34].\n",
    "    - For example, Cai et al. [3] proposed a hierarchical clustering method that exploits\n",
    "        - visual, \n",
    "        - textual, and \n",
    "        - link analysis.\n",
    "    - By using block-level link analysis techniques, an image graph is constructed. They then applied spectral techniques to find a Euclidean embedding of the images.\n",
    "        - As a result, each image has three types of representations: \n",
    "            - visual feature, \n",
    "            - textual feature, and \n",
    "            - graph-based representation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Tag Mining\n",
    "* 3.1 Tag Ranking\n",
    "* 3.2 Tag Refinement\n",
    "* 3.3 Tag Information Enrichment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"red\">In newly emerging social media sharing services, such as the Flickr and Youtube, users are encouraged to share multimedia data on the Web and annotate content with tags.</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The tags can be used to index multimedia data and support efficient tag-based search."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The existing works mainly focus on the following three aspects: \n",
    "* (a) tag ranking, \n",
    "    - which aims to differentiate the tags associated with the images with various levels of relevance; \n",
    "* (b) tag refinement \n",
    "    - with the purpose to refine the unreliable human-provided tags; and \n",
    "* (c) tag information enrichment, \n",
    "    - which aims to supplement tags with additional information [26]."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Tag Ranking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* As shown in [25], the relevance level of the tags cannot be distin- guished from the tag list of an image. \n",
    "* <font color=\"red\">The lack of relevance information in the tag list</font> has limited the application of tags."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Works\n",
    "* Liu et al. [25] \n",
    "    - proposed to estimate tag relevance scores using kernel density estimation, \n",
    "    - and then employ random walk to boost this primary estimation.\n",
    "* Li et al. [22] \n",
    "    - proposed a data driven method for tag ranking. \n",
    "    - They learned the relevance scores of tags by a neighborhood voting approach.\n",
    "        - Given an image and one of its associated tag, the relevance score is learned by accumulating the votes from the visual neighbors of the image.\n",
    "    - multiple visual spaces [23]\n",
    "* score fusion or rank fusion method\n",
    "    - They learned the relevance scores of tags and ranked them by neighborhood voting in different feature spaces, and the results are aggregated with a score fusion or rank fusion method\n",
    "    - Borda count and RankBoost\n",
    "        * Different aggregation methods have been investigated, such as the average score fusion, Borda count and RankBoost."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Tag Refinement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"red\">User-provided tags are often noisy and incomplete.</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tag refinement technologies are proposed aiming at obtaining more accurate and complete tags for multimedia description"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/cap11.4.png\" width=600 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A lot of tag refinement approaches have been developed based on various statistical learning techniques. Most of them are based on the following three assumptions.\n",
    "* The refined tags <font color=\"red\">should not change too much from those provided by the users</font>. \n",
    "    - This assumption is usually used to <font color=\"blue\">regularize</font> the tag refinement.\n",
    "* <font color=\"red\">The tags of visually similar images should be closely related</font>. \n",
    "    - This is a natural assumption that most <font color=\"blue\">automatic tagging</font> methods are also built upon.\n",
    "* <font color=\"red\">Semantically close or correlative tags should appear with high correlation</font>. \n",
    "    - For example, when a tag “sea” exists for an image, the tags “beach” and “water” should be assigned with higher confi- dence while the tag “street” should have low confidence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Works\n",
    "* Chen et al. [6] \n",
    "    - first trained a SVM classifier for each tag with the loosely labeled positive and negative samples. \n",
    "    - The classifiers are used to estimate the initial relevance scores of tags.\n",
    "    - They then refined the scores with a graph-based method \n",
    "        - that simultaneously considers the similarity between images and semantic correlation among tags.\n",
    "* Xu et al. [52]\n",
    "    - proposed a tag refinement algorithm from topic modeling point of view.\n",
    "    - regularized latent Dirichlet allocation (rLDA)\n",
    "* Zhu et al. [64]\n",
    "    - proposed a matrix decomposition method. \n",
    "    - They used a matrix to represent the image-tag relationship\n",
    "* Fan et al. [8] \n",
    "    - grouped images with a target tag into clusters. \n",
    "    - Each cluster is regarded as a unit. \n",
    "    - The initial relevance scores of the clusters are estimated and then refined by a random walk process.\n",
    "* Liu et al. [24] \n",
    "    - adopted a three-step approach. \n",
    "        - The first step filters out tags that are intrinsically content-unrelated based on the ontology in WordNet. \n",
    "        - The second step refines the tags based on the consistency of visual similarity and semantic similarity of images. \n",
    "        - The last step performs tag enrichment, which expands the tags with their appropriate synonyms and hypericum."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 Tag Information Enrichment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* In the manual tagging process, generally human labelers will only assign appropriate tags to multimedia entities without any additional information, such as the image regions depicted by the corresponding tags. \n",
    "* But <font color=\"red\">by employing computer vision and machine learning technologies</font>, certain information of the tags, such as the descriptive regions and saliency, <font color=\"red\">can be automatically obtained</font>.\n",
    "* We refer to these as tag information enrichment.\n",
    "    - Most existing works employ the following two steps for tag information enrichment. \n",
    "        - First, <font color=\"red\">tags</font> are <font color=\"red\">localized</font> into regions of images or sub-clips of videos. \n",
    "        - Second, the characteristics of the regions or sub-clips are <font color=\"red\">analyzed</font>, \n",
    "            - and the information about the tags is enriched accordingly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/cap11.5.png\" width=600 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Works\n",
    "* Liu et al. [28] \n",
    "    - proposed a method to locate image tags to corresponding regions. \n",
    "        - They first performed over-segmentation to decompose each image into patches \n",
    "        - and then discovered the relationship between patches and tags via sparse coding. \n",
    "        - The over-segmented regions are then merged to accomplish the tag-to-region process.\n",
    "    - Liu et al. extended the approach based on image search [29].\n",
    "        - For a tag of the target image, they collected a set of images by using the tag as query with an image search engine. \n",
    "        - They then learned the relationship between the tag and the patches in this image set.\n",
    "    - Liu et al. [27] accomplished the tag-to-region task \n",
    "        - by regarding an image as a bag of regions \n",
    "        - and then performed tag propagation on a graph, \n",
    "            - in which vertices are images and edges are constructed based on the visual link of regions.\n",
    "* Feng et al. [10] \n",
    "    - proposed a tag saliency learning scheme,\n",
    "        - which is able to rank tags according to their saliency levels to an image’s content.\n",
    "        - They first located tags to images’ regions with a multi-instance learning approach.\n",
    "            - In multi-instance learning, an image is regarded as a bag of multiple instances, i.e., regions [58]\n",
    "        - They then analyzed the saliency values of these regions.\n",
    "* Yang et al. [55] \n",
    "    - proposed a method to associate a tag with a set of properties, \n",
    "        - including location, color, texture, shape, size and dominance.\n",
    "        - They employed a multi-instance learning method \n",
    "            - to establish the region that each tag is corresponding to, \n",
    "            - and the region is then analyzed to establish the properties, \n",
    "                - as shown in Figure 11.5 (b).\n",
    "* Sun and Bhowmick [41] \n",
    "    - defined a tag’s visual representativeness\n",
    "        - based on a large image set and the subset that is associated with the tag. \n",
    "    - They employed two distance metrics,\n",
    "        - cohesion and \n",
    "        - separation, \n",
    "            - to estimate the visual representativeness measure.\n",
    "* Ulges et al. [43] \n",
    "    - proposed an approach to localize video-level tags to keyframes. \n",
    "        - Given a tag, it regards whether a keyframe is relevant as a latent random variable. \n",
    "        - An EM-style process is then adopted to estimate the variables.\n",
    "* Li et al. [21] \n",
    "    - employed a multi-instance learning approach to accomplish the video tag localization, \n",
    "        - in which video and shot are regarded as bag and shot, respectively."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Joint Text and Visual Content Mining\n",
    "* 4.1 Visual Re-ranking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"red\">The integration of text and visual content has been found to be more effective than exploiting purely text or visual content separately.</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* The joint text and content mining in multimedia retrieval often comes down to finding effective mechanisms for <font color=\"red\">fusing multi-modality information</font> from textual metadata and visual content.\n",
    "* Existing research efforts can generally be categorized into four paradigms: \n",
    "    - (a) linear fusion; \n",
    "    - (b) latent-space-based fusion; \n",
    "    - (c) graph-based fusion; and \n",
    "    - (d) visual re-ranking \n",
    "        - that exploits visual information to refine text-based retrieval results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear fusion \n",
    "* Linear fusion combines the retrieval results from various modalities linearly [18][4][31].\n",
    "* In [18], \n",
    "    - visual content and text are combined in both online learning stage with relevance feedback and offline keyword propagation. \n",
    "* In [31], \n",
    "    - linear, max, and average fusion strategies are employed to aggregate the search results from visual and textual modalities. \n",
    "* Chang et al. [4] \n",
    "    - adopted a query-class-dependent fusion approach. \n",
    "* <font color=\"red\">The critical task in linear fusion is the estimation of fusion weights of different modalities.</font>\n",
    "* Jing and Baluja[17]\n",
    "    - proposed a <font color=\"red\">VisualRank</font> framework to efficiently model similarity of Google image search results with graph [17]. \n",
    "    - The framework casts the re-ranking problem\n",
    "        - as random walk on an affinity graph and \n",
    "        - reorders images according to the visual similarities. \n",
    "    - The final result list is generated via sorting the images based on graph nodes’ weights."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The latent space based fusion \n",
    "* The laten space based fusion assumes that\n",
    "    - <font color=\"red\">there is a latent space \n",
    "        - shared by different modalities</font> \n",
    "    - and thus unify different modalities \n",
    "        - by transferring \n",
    "            - the features of these modalities into \n",
    "            - the shared latent space [63][62]. \n",
    "* Zhao et al. [63] \n",
    "    - adopted the Latent Semantic Indexing (LSI) method to fuse text and visual content. \n",
    "        - Zhang et al. [62] \n",
    "            - proposed a probabilistic context model to explicitly exploit the synergy between text and visual content. \n",
    "            - The synergy is represented as a hidden layer between the image and text modalities."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Graph based approach \n",
    "* Graph based approach [49] \n",
    "    - first builds the relations between different modalities, \n",
    "        - such as relations between images and text using the Web page structure. \n",
    "    - The relations are then utilized to iteratively update the similarity graphs computed from different modalities. \n",
    "* <font color=\"red\">The difficulty of creating similarity graphs for billions of images on the Web makes this approach insufficiently scalable.</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1 Visual Re-ranking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visual re-ranking is emerging as one of the promising technique for automated boosting of retrieval precision [42] [30] [55]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* In particular, \n",
    "    - given a textual query, \n",
    "    - an <font color=\"red\">initial list of multimedia entities</font> is returned <font color=\"red\">using the text-based retrieval scheme</font>.\n",
    "    - Subsequently, the most relevant results are moved to the top of the result list while the less relevant ones are <font color=\"red\">reordered</font> to the lower ranks. \n",
    "    - As such, the overall search precision at the top ranks can be enhanced dramatically."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* According to the statistical analysis model used, the <font color=\"red\">existing re-ranking approaches</font> can roughly be categorized into <font color=\"red\">three categories</font> including \n",
    "    - the clustering based, \n",
    "    - classification based and \n",
    "    - graph based methods."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### clustering based\n",
    "* Cluster analysis is very useful to <font color=\"red\">estimate the inter-entity similarity</font>.\n",
    "* The clustering based re-ranking methods stem from the key observation that <font color=\"red\">a lot of visual characteristics can be shared by relevant images or video clips</font>\n",
    "* mean-shift, K- means, and K-medoids etc..\n",
    "* Hsu et al. [16]\n",
    "    - One good example of clustering based re-ranking algorithms is an Information Bottle based scheme developed by Hsu et al. [16].\n",
    "    - Its main objective is to identify optimal clusters of images that can minimize the loss of mutual information.\n",
    "* In [19]\n",
    "    - a fast and accurate scheme is proposed for grouping Web image search results into semantic clusters.\n",
    "    - For a given query, a few related semantic clusters are identified in the first step. \n",
    "    - Then, the cluster names relating to query are derived and used as text keywords for querying image search engine. \n",
    "    - The empirical results from a set of user studies demonstrate an improvement in performance over Google image search results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### classification based\n",
    "* In the classification based methods, visual re-ranking is formulated <font color=\"red\">as a binary classification problem</font> aiming to identify <font color=\"red\">whether each search result is relevant or not</font>.\n",
    "* The major process for result list reordering consists of three major steps: \n",
    "    - (a) the selection of pseudo-positive and pseudo-negative samples; \n",
    "    - (b) use the samples obtained in step (a) to train a classification scheme; and \n",
    "    - (c) reorder the samples according to their relevance scores given by the trained classifier.\n",
    "    - <font color=\"red\">pseudo relevance feedback (PRF)</font>\n",
    "        - For existing classifi- cation methods, pseudo relevance feedback (PRF) is applied to select the training examples. \n",
    "        - It assumes that: \n",
    "            - (a) a limited number of top-ranked entities in the initial retrieval results are highly relevant to the search queries; and \n",
    "            - (b) automatic local analysis over the entities can be very helpful to refine query representation.\n",
    "* In [54], \n",
    "    - the query images or video clip examples are used as the pseudo-positive samples. \n",
    "    - The pseudo-negative samples are selected from either the least relevant samples in the initial result list or the databases that contain less samples related to the query. \n",
    "    - The second step of the classification based methods aim to train classifiers and a wide range of statistical classifiers can be adopted. \n",
    "        - They include the Support Vector Machine (SVM) [54], Boosting [53] and ListNet [57].\n",
    "* <font color=\"red\">However, in many real scenarios, the training examples obtained via PRF are very noisy and might not be adequate for training effective classifier.</font>\n",
    "* Fergus et al. [11] \n",
    "    - Fergus et al. [11] used RANSAC to sample a training subset with a high percentage of relevant images.\n",
    "    - A generative constellation model is learned for the query category while a background model is learned from the query “things”. \n",
    "    - Images are re-ranked based on their likelihood ratio.\n",
    "* Schroff et al. [35] \n",
    "    - first learned a query independent text based re-ranker. \n",
    "    - The top ranked results from the text based re-ranking are then selected as positive training examples. \n",
    "    - Negative training examples are picked randomly from the other queries. \n",
    "    - A binary SVM classifier is then used to re-rank the results on the basis of visual features.\n",
    "* Wang et al. [44] \n",
    "    - learned a generative text model from the query’s Wikipedia 4 page and a discriminative image model from the Caltech [15] and Flickr data sets. \n",
    "    - Search results are then re-ranked on the basis of these learned probability models. Some user interactions are required to disambiguate the query."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### graph based\n",
    "* Graphs provide a natural and comprehensive way to <font color=\"red\">explore complex relations between data at different levels</font> and have been applied to a wide range of applications [59][46][47][60].\n",
    "* With the graph based re-ranking methods, \n",
    "    - the multimedia entities in top ranks and their associations/dependencies can be represented as a collection of nodes (vertices) and edges.\n",
    "* In [16], Hsu et al. \n",
    "    - modeled the re-ranking process as a random walk over the context graph. \n",
    "    - In order to effectively leverage the retrieved results from text search, \n",
    "    - each sample corresponds to a “dongle” node containing ranking score based on text. \n",
    "    - For the framework, edges between “dongle” nodes are weighted with multi-modal similarities.\n",
    "* <font color=\"red\">In many cases, the structure of large scale graphs can be very complex and this easily makes related analysis process very expensive in terms of computational cost.</font>\n",
    "* Jing and Baluja[17]\n",
    "    - proposed a <font color=\"red\">VisualRank</font> framework to efficiently model similarity of Google image search results with graph [17]. \n",
    "    - The framework \n",
    "        - casts the re-ranking problem as random walk on an affinity graph and \n",
    "        - reorders images according to the visual similarities. \n",
    "        - The final result list is generated via sorting the images based on graph nodes’ weights.\n",
    "* In [42], Tian et al., \n",
    "    - presented a Bayesian video search re-ranking framework \n",
    "        - formulating the re-ranking process as an energy minimization problem. \n",
    "    - The main design goal is to \n",
    "        - optimize the consistency of ranking scores over visually similar videos and \n",
    "        - minimize the disagreement between the optimal list and the initial list.\n",
    "* <font color=\"red\">Indeed, graph analysis has been shown to be a very powerful tool for analyzing and identifying salient structure and useful patterns inside the visual search results.</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Cross Text and Visual Content Mining"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* However, <font color=\"red\">in some real world applications, images may not always have associated text</font>. \n",
    "    - For example, most surveillance images/videos in in-house repository are not accompanied with any text. \n",
    "    - Even on social media Website such as the Flickr, there exist a substantial number of images without any tags.\n",
    "* In such cases, <font color=\"red\">joint text and visual content mining cannot be applied due to missing text modality.</font>\n",
    "* Recently, <font color=\"red\">cross text and visual content mining</font> has been studied in the context of transfer learning techniques. This class of techniques emphasizes the <font color=\"red\">transferring of knowledge across different domains or tasks</font> [32]."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cross text and visual content mining does not require that a test image has an associated text modality, and is thus <font color=\"red\">beneficial to dealing with the images without any text by propagating the semantic knowledge from text to images</font>\n",
    "* It is also motivated by two observations. \n",
    "- First, visual content of images is much more complicated than the text feature. \n",
    "    - While the textual words are easier to interpret, there exist a tremendous semantic gap between visual content and high-level semantics. \n",
    "- Second, image understanding becomes particularly challenging when only a few labeled images are available for training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, it is not trivial to transfer knowledge between various domains/tasks due to the following <font color=\"red\">challenges</font>:\n",
    "* The target data may be drawn from a <font color=\"red\">distribution different from the source data</font>.\n",
    "* The target and source data may be in <font color=\"red\">different feature spaces</font> (e.g., image and text) and there may be <font color=\"red\">no correspondence</font> between instances in these spaces.\n",
    "* The target and source tasks may have <font color=\"red\">different output spaces</font>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Figure 11.6 from [56] presents an intuitive illustration of four learning paradigms, including \n",
    "* traditional machine learning, \n",
    "* transfer learning across different distributions, \n",
    "* multi-view learning and \n",
    "* heterogenous transfer learning. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/cap11.6.png\" width=600 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* As we can see, heterogenous transfer learning is usually much more challenging due to the unknown correspondence across the distinct feature spaces. \n",
    "* In order to learn the underlying correspondence for knowledge transformation, a “<font color=\"red\">semantic bridge</font>” is required."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Works\n",
    "* Most existing works exploit the tag information that provide text-to-image linking information.\n",
    "* Dai et al. [7] \n",
    "    - showed that such information can be effectively leveraged for transferring knowledge between text and images. \n",
    "    - The key idea of [7] is to construct a correspondence between the images and the auxiliary text data with the use of tags.\n",
    "    - Probabilistic latent semantic analysis (PLSA) model is employed to construct a latent semantic space which can be used for transferring knowledge.\n",
    "* Chen et al. [56] \n",
    "    - proposed the concept of heterogeneous transfer learning and applied it to improve image clustering by leveraging auxiliary text data. \n",
    "    - They collected annotated images from the social web, \n",
    "    - and used them to construct a text to image mapping. \n",
    "    - The algorithm is referred to as aPLSA (Annotated Probabilistic La- tent Semantic Analysis). \n",
    "    - The key idea is to unify two different kinds of latent semantic analysis in order to create a bridge between the text and images.\n",
    "        - The first kind of technique performs PLSA analysis on the target images, which are converted to an image instance-to-feature co- occurrence matrix.\n",
    "        - The second kind of PLSA is applied to the annotated image data from social Web, which is converted into a text-to-image feature co-occurrence matrix.\n",
    "    - In order to unify those two separate PLSA models, these two steps are done simultaneously with common latent variables used as a bridge linking them.\n",
    "* Qi et al. [33] \n",
    "    - proposed to learn a “translator” which can directly establish the semantic correspondence between text and images \n",
    "        - even if they are new instances of the image data with unknown correspondence to the text articles.\n",
    "    - This capability increase the flexibility of the approach and makes it more widely applicable. \n",
    "        - Specifically, they created a new topic space into which both the text and images are mapped. \n",
    "    - A translator is then learned to link the instances across heterogeneous text and image spaces. \n",
    "    - With the resultant translator, the semantic labels can be propagated from any labeled text corpus to any new image by a process of cross-domain label propagation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Summary and Open Issues\n",
    "* Joint text and visual content multimedia ranking\n",
    "* Scalable text mining for large-scale multimedia management\n",
    "* Multimedia social network mining"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although research efforts in this filed have made great progress in various aspects, there are still many open research issues that need to be explored."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Joint text and visual content multimedia ranking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Despite the success of visual re-ranking in multimedia retrieval, visual re-ranking <font color=\"red\">only employs the visual content to refine text-based retrieval results</font>; \n",
    "* visual content has not been used to assist in learning the ranking model of search engine, and sometimes it is only able to bring in limited performance improvements.\n",
    "    -  In particular, if text-based ranking model is biased or over-fitted, re-ranking step will suffer from the error that is propagated from the initial results, and thus the performance improvement will be negatively impacted."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scalable text mining for large-scale multimedia management"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Despite of the success of existing text mining in multimedia, most existing techniques <font color=\"red\">suffer from difficulties in handling large-scale multimedia data</font>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multimedia social network mining"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Multimedia social networking is becoming an important part of media consumption for Internet users. \n",
    "* It brings in new and rich metadata, such as user preferences, interests, behaviors, social relationships, and social network structure etc.\n",
    "* Numerous research topics can be explored, including \n",
    "    - (a) the combination of conventional techniques with information derived from social network communities; \n",
    "    - (b) fusion analysis of content, text, and social network data; and \n",
    "    - (c) personalized multimedia analysis in social networking environments."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 참고자료\n",
    "* [] 주는 (1) 책에 나오는 레퍼런스 번호\n",
    "* (1) Mining Text Data - http://link.springer.com/book/10.1007/978-1-4614-3223-4/page/1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
