{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. A Survey of Text Summarization Techniques"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Contents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. How do Extractive Summarizers Work?\n",
    "2. Topic Representation Approaches\n",
    "3. Influence of Context \n",
    "4. Indicator Representations and Machine Learning for Summarization \n",
    "5. Selecting Summary Sentences \n",
    "6. Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. How do Extractive Summarizers Work?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"red\">Intermediate representation</font> -> <font color=\"red\">Score sentences</font> -> <font color=\"red\">Select summary sentences</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Summarization systems need to produce a concise and fluent summary conveying the key information in the input.\n",
    "* we constrain our discussion to extractive summarization systems for short, paragraph-length summaries and explain how these systems perform summarization.\n",
    "* we distinguish three relatively independent tasks performed by virtually all summarizers:\n",
    "    - <font color=\"red\">Intermediate representation</font>\n",
    "        - Even the simplest systems derive some intermediate representation of the text they have to summarize and identify important content based on this representation.\n",
    "        - <font color=\"blue\">Topic representation approaches</font> \n",
    "            - convert the text to an intermediate representation interpreted as the topic(s) discussed in the text.\n",
    "            - <font color=\"orange\">frequency-driven approaches</font>\n",
    "                - They include \n",
    "                    - frequency, \n",
    "                    - TF.IDF and \n",
    "                    - topic word approaches in which the topic representation consists of \n",
    "                        - a simple table of words and \n",
    "                        - their corresponding weights, \n",
    "                            - with more highly weighted words being more indicative of the topic.\n",
    "            - <font color=\"orange\">lexical chain approaches</font>\n",
    "                - in which a thesaurus such as WordNet is used to find \n",
    "                    - topics or \n",
    "                    - concepts of semantically related words and then give\n",
    "                    - weight to the concepts\n",
    "            - <font color=\"orange\">latent semantic analysis</font> \n",
    "                - in which patterns of word co-occurrence are \n",
    "                - identified and \n",
    "                - roughly construed as topics, \n",
    "                - as well as weights for each pattern\n",
    "            - <font color=\"orange\">full blown Bayesian topic models</font>\n",
    "                - in which the input is represented as \n",
    "                - a mixture of topics and \n",
    "                - each topic is given as a table of word probabilities (weights) for that topic. \n",
    "        - <font color=\"blue\">Indicator representation approaches</font> \n",
    "            - represent each sentence in the input \n",
    "            - as a list of indicators of \n",
    "                - importance such as \n",
    "                    - sentence length, \n",
    "                    - location in the document, \n",
    "                    - presence of certain phrases, etc. \n",
    "            - In graph models, such as LexRank, \n",
    "                - the entire document is represented as a network of inter-related sentences.\n",
    "        - <font color=\"red\">Score sentences</font>\n",
    "            - Once an intermediate representation has been de- rived, each sentence is assigned a score which indicates its importance.\n",
    "            - <font color=\"blue\">For topic representation approaches</font>, \n",
    "                - the score is commonly related to \n",
    "                    - how well a sentence expresses some of the most important topics in the document or to \n",
    "                    - what extent it combines information about different topics.\n",
    "            - <font color=\"blue\">For the majority of indicator representation methods</font>, \n",
    "                - the weight of each sentence is determined by \n",
    "                    - combining the evidence from the different indicators\n",
    "                    - most commonly by using machine learning techniques to discover indicator weights.\n",
    "                - In LexRank, \n",
    "                    - the weight of each sentence is derived by \n",
    "                        - applying stochastic techniques \n",
    "                        - to the graph representation of the text.    \n",
    "    - <font color=\"red\">Select summary sentences</font>\n",
    "        - the summarizer has to select the best combination of important sentences to form a paragraph length summary. \n",
    "        - In the best n approaches, \n",
    "            - the top n most important sentences which \n",
    "            - combined have the desired summary length are selected to form the summary.\n",
    "        - In maximal marginal relevance approaches, \n",
    "            - sentences are selected in an \n",
    "            - iterative greedy procedure. \n",
    "                - At each step of the procedure \n",
    "                    - the sentence importance score is recomputed \n",
    "                    - as a linear combination between \n",
    "                        - the original importance weight of the sentence and \n",
    "                        - its similarity with already chosen sentences.\n",
    "        - In global selection approaches, \n",
    "            - the optimal collection of sentences is selected subject to constraints that try to \n",
    "                - maximize overall importance, \n",
    "                - minimize redundancy, and, for some approaches, \n",
    "                - maximize coherence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Topic Representation Approaches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 2.1 Topic Words \n",
    "* 2.2 Frequency-driven Approaches \n",
    "* 2.3 Latent Semantic Analysis \n",
    "* 2.4 Bayesian Topic Models \n",
    "* 2.5 Sentence Clustering and Domain-dependent Topics "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Topic Words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* H1: P(w|I) = P(w|B) = p (w is not descriptive)\n",
    "* H2: P(w|I) = pI and P(w|B) = pB and pI > pB (w is descriptive)\n",
    "    - input I \n",
    "    - background corpus B\n",
    "    - two assumptions:\n",
    "        - (H1) that the probability of a word in the input is the same as in the background B\n",
    "        - (H2) that the word has a different, higher probability, in the input than in the background."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* The likelihood of a text with respect to a given word of interest, w,\n",
    "    - binomial distribution formula\n",
    "    - as a sequence of words wi: w1w2 . . . wN .\n",
    "    - The occurrence of each word is a Bernoulli trial with probability p of success, which occurs when wi = w."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* The overall probability of observing the word w appearing k times in the N trials is given by the binomial distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/eq3.1.png\" width=600 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* For H1, \n",
    "    - the probability p is computed from \n",
    "        - the input and \n",
    "        - the background collection taken together. \n",
    "* For H2, \n",
    "    - p1 is computed from the input, \n",
    "    - p2 from the background, and \n",
    "    - the likelihood of the entire data is \n",
    "        - equal to the product of the binomial for the input and that for the background."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* the likelihood ratio is defined as"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/eq3.2.png\" width=600 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* where the counts with subscript I are computed only from the input to the summarizer and \n",
    "* those with index B are computed over the background corpus."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Topic signature "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* The statistic equal to −2logλ has a known statistical distribution (χ2), which can be used to determine which words are topic signatures.\n",
    "* Topic signature words are those that have a <font color=\"red\">likelihood statistic greater than what one would expect by chance</font>.\n",
    "    - The probability of obtaining a given value of the statistic purely by chance can be looked up in a χ2 distribution table; \n",
    "    - for instance a value of 10.83 can be obtained by chance with probability of 0.001."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The importance of a sentence is computed as "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* the <font color=\"red\">number of topic signatures</font> it contains or as \n",
    "* the <font color=\"blue\">proportion of topic signatures</font> in the sentence.\n",
    "* Both of these sentence scoring functions \n",
    "    - are based on the same topic representation, \n",
    "    - the scores they assign to sentences may be rather different. \n",
    "    - <font color=\"red\">The first approach</font> \n",
    "        - is likely to score \n",
    "            - longer sentences higher, simply \n",
    "            - because they contain more words. \n",
    "    - <font color=\"blue\">The second approach</font> \n",
    "        - favors density of topic words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Frequency-driven Approaches "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Word probability\n",
    "* TF*IDF weighting "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In principle it would even be beneficial to be able to compare the continuous weights of words and determine which ones are more related to the topic."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* we present in this section— word probability and TF.IDF—indeed assign non-binary weights related on the number of occurrences of a word or concept.\n",
    "* Research has already shown that the binary weights give more stable indicators of sentence importance than word probability and TF.IDF.\n",
    "* Nonetheless we overview these approaches because of their conceptual simplicity and reasonable performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word probability"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* The probability of a <font color=\"red\">word w</font>, <font color=\"red\">p(w)</font> \n",
    "    - is computed from the input, which can be a cluster of related documents or a single document. \n",
    "* It is calculated as the <font color=\"red\">number of occurrences of a word</font>, <font color=\"red\">c(w)</font> \n",
    "* divided by the <font color=\"red\">number of all words in the input</font>, <font color=\"red\">N</font>:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/eq3.3.png\" width=600 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SUMBASIC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* SUMBASIC is one system developed to operationalize the idea of using frequency for sentence selection.\n",
    "* For each sentence Sj \n",
    "    - in the input it assigns a weight \n",
    "        - equal to the average probability p(wi) \n",
    "            - of the content words in the sentence, \n",
    "* estimated from the input for summarization:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/eq3.4.png\" width=600 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF*IDF weighting (Term Frequency x Inverse Document Frequency)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* The only additional information besides the term frequency c(w) \n",
    "    - that we need in order to compute the weight of \n",
    "        - a word w which appears <font color=\"red\">c(w) times in the input</font> for summarization is \n",
    "        - <font color=\"red\">the number of documents, d(w)</font>, \n",
    "        - <font color=\"red\">in a background corpus of D documents</font> that contain the word. \n",
    "* This allows us to compute the inverse document frequency:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/eq3.5.png\" width=600 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* In many cases c(w) is divided by the maximum number of occurrences of any word in the document, which normalizes for document length.\n",
    "* Descriptive topic words are those that appear often in a document, but are not very common in other documents. \n",
    "* Words that appear in most documents will have an IDF close to zero."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Lexical chains"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* and some related approaches represent topics that are discussed throughout a text by <font color=\"red\">exploiting relations between words</font>.\n",
    "* The lexical chains approach captures the intuition that topics are expressed using not a single word but instead different related words.\n",
    "    - <font color=\"blue\">For example, the occurrence of the words “car”, “wheel”, “seat”, “passenger” indicates a clear topic, even if each of the words is not by itself very frequent</font>.\n",
    "    - The approach heavily relies on <font color=\"red\">WordNet</font>, \n",
    "        - a manually compiled thesaurus which lists the different senses of each word, \n",
    "        - as well as word relationships such as \n",
    "            - synonymy, \n",
    "            - antonymy, \n",
    "            - part-whole and \n",
    "            - general-specific."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Latent Semantic Analysis "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* <font color=\"red\">Latent semantic analysis (LSA)</font> is \n",
    "    - a <font color=\"blue\">robust unsupervised technique</font> for deriving \n",
    "    - an <font color=\"blue\">implicit representation</font> of \n",
    "    - <font color=\"blue\">text semantics</font> based on observed \n",
    "    - <font color=\"blue\">co-occurrence of words</font>.\n",
    "* Gong and Liu proposed the use of LSA for \n",
    "    * single and multi-document generic summarization of news, as a way of\n",
    "    * identifying important topics in documents \n",
    "    * <font color=\"red\">without the use of lexical resources such as WordNet</font>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Building the topic representation starts by \n",
    "    - filling in a n by <font color=\"red\">m matrix A</font>: \n",
    "        - each row corresponds to a word from the input <font color=\"blue\">(n words)</font> and \n",
    "        - each column corresponds to a sentence in the input <font color=\"blue\">(m sentences)</font>. \n",
    "        - <font color=\"blue\">Entry aij</font> of the matrix corresponds to the <font color=\"blue\">weight of word i in sentence j</font>.\n",
    "            - If the sentence does <font color=\"orange\">not contain the word, the weight is zero</font>,\n",
    "            - <font color=\"orange\">otherwise</font> the weight is equal to <font color=\"orange\">the TF*IDF weight</font> of the word."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Standard techniques for singular value decomposition (SVD) \n",
    "    - from linear algebra are applied to \n",
    "    - the matrix A, \n",
    "        - to represent it as the product of three matrices: <font color=\"red\">A = U Σ V.T</font> .\n",
    "            - <font color=\"red\">Matrix U</font> is a n by m matrix of real numbers. \n",
    "                - <font color=\"blue\">Each column</font> can be interpreted as a <font color=\"blue\">topic</font>, i.e. a specific combination of words from the input with the weight of each word in the topic given by the real number. \n",
    "            - <font color=\"red\">Matrix Σ</font> is diagonal m by m matrix. \n",
    "                - The single entry in <font color=\"blue\">row i</font> of the matrix corresponds to the <font color=\"blue\">weight of the “topic”</font>, which is the <font color=\"blue\">ith column of U</font>. \n",
    "                - Topics with low weight can be ignored, by deleting the last k rows of U, the last k rows and columns of Σ and the last k rows of V.T . \n",
    "                - This procedure is called <font color=\"orange\">dimensionality reduction</font>.\n",
    "            - <font color=\"red\">Matrix V.T</font> is a \n",
    "                - <font color=\"blue\">new representation of the sentences</font>, \n",
    "                - one sentence per row, each of which is expressed not in \n",
    "                    - terms of words that occur in the sentence but rather in \n",
    "                    - terms of the topics given in U . \n",
    "            - <font color=\"red\">The matrix D = ΣV.T</font> combines \n",
    "                - the <font color=\"blue\">topic weights</font> and \n",
    "                - the <font color=\"blue\">sentence representation</font> to indicate to what extent the sentence conveys the topic, with \n",
    "                - <font color=\"blue\">dij</font> indicating the <font color=\"blue\">weight for topic i in sentence j</font>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Another improvement was to notice that often <font color=\"red\">sentences that discuss several of the important topics are good candidates for summaries</font>.\n",
    "* To identify such sentences, the weight of sentence si is set to equal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/eq3.6.png\" width=600 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4 Bayesian Topic Models "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* The original <font color=\"red\">Bayesian model</font> for multi-document summarization, derives <font color=\"red\">several distinct probabilistic distributions</font> of words that appear in the input.\n",
    "    - One distribution is for <font color=\"blue\">general English (G)</font>, \n",
    "    - one for the <font color=\"blue\">entire cluster to be summarized (C)</font> and \n",
    "    - one for each <font color=\"blue\">individual document i</font> in that <font color=\"blue\">cluster (Di)</font>. \n",
    "    - Each of G, C and D consist of \n",
    "        - <font color=\"orange\">tables of words</font> and \n",
    "        - their <font color=\"orange\">probabilities, or weights</font>, \n",
    "        - much like the word probability approach, but \n",
    "        - the weights are very different in G, C and D: \n",
    "            - a word with high probability in general English is likely to have (almost) zero weight in the cluster table C. \n",
    "        - The tables (probability distributions) are derived as a part of a hierarchical topic model. \n",
    "* It is an <font color=\"red\">unsupervised model</font> and the only data it requires are several multi-document clusters; \n",
    "* the <font color=\"red\">general English weights reflect occurrence of words across</font> most of the input clusters.\n",
    "* The topic model representations are quite appealing because they capture information that is lost in most of the other approaches.\n",
    "    - They, for example, have an \n",
    "        - <font color=\"red\">explicit representation of the individual documents</font> that make up the cluster that is to be summarized\n",
    "    - It is also flexible in the manner in which it derives \n",
    "        - the general English weights of words, \n",
    "        - <font color=\"red\">without</font> the need for \n",
    "            - a <font color=\"red\">pre-determined stop word list</font>, or \n",
    "            - <font color=\"red\">IDF values</font> from a background corpus."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Kullback-Lieber (KL) divergence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* In addition to the improved representation, the topic models highlight the use of a different sentence scoring procedure : Kullback-Lieber (KL) divergence.\n",
    "* In general the KL divergence of probability distribution Q with respect to distribution P over words w is defined as"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/eq3.7.png\" width=600 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* P(w) and Q(w) are the probabilities of w in P and Q respectively."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Sentences are scored and selected in a greedy iterative procedure. \n",
    "    - In each iteration \n",
    "        - the best sentence i to be selected \n",
    "        - in the summary is determined \n",
    "        - as the one for which the KL divergence \n",
    "        - between C, \n",
    "            - the probabilities of words \n",
    "            - in the cluster to be summarized, \n",
    "            - and the summary so far, including i, is smallest."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.5 Sentence Clustering and Domain-dependent Topics "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* In multi-document summarization of <font color=\"red\">news</font>, \n",
    "    - the input by definition consists of <font color=\"blue\">several articles</font>, \n",
    "        - possibly from <font color=\"blue\">different sources</font>, \n",
    "        - on the <font color=\"blue\">same topic</font>. \n",
    "    - <font color=\"red\">Across the different articles</font> \n",
    "        - there will be <font color=\"blue\">sentences</font> that <font color=\"blue\">contain similar information</font>.\n",
    "    - <font color=\"red\">Information</font> that <font color=\"blue\">occurs in many</font> of the input documents is likely <font color=\"blue\">important</font> and worth selecting in a summary. \n",
    "    - Of course, <font color=\"blue\">verbatim repetition</font> on the sentence level is <font color=\"blue\">not that common across sources</font>. \n",
    "    - Rather, <font color=\"red\">similar sentences</font> can be <font color=\"red\">clustered together</font>. \n",
    "* In summarization, \n",
    "    - <font color=\"red\">cosine similarity</font> is standardly used \n",
    "        - to measure the similarity between the vector representations of sentences."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentence Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Below is an example of a sentence cluster from different documents in the input to a multi-document summarizer. <font color=\"red\">All four sentences share common content</font> that should be conveyed in the summary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>S1</b> PAL was devastated by a pilots’ strike in June and by the region’s currency crisis.\n",
    "\n",
    "<b>S2</b> In June, PAL was embroiled in a crippling three-week pilots’ strike.\n",
    "\n",
    "<b>S3</b> Tan wants to retain the 200 pilots because they stood by him when the majority of PAL’s pilots staged a devastating strike in June.\n",
    "\n",
    "<b>S4</b> In June, PAL was embroiled in a crippling three-week pilots’ strike."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Constraining <font color=\"red\">each sentence to belong to only one cluster is a distinct disadvantage of the sentence clustering approach</font>, and graph methods for summarization which we discuss in the next section, have proven to exploit the same ideas in a more flexible way."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### domain-specific summarization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* For <font color=\"red\">domain-specific summarization</font>, however, \n",
    "* clustering of sentences \n",
    "    - from <font color=\"red\">many samples</font> \n",
    "    - from the <font color=\"red\">domain</font> \n",
    "* can give a good indication \n",
    "    - about the <font color=\"red\">topics</font> \n",
    "    - that are <font color=\"red\">usually discussed in the domain</font>, \n",
    "    - and the type of information \n",
    "        - that a summary would need to convey. \n",
    "* In this case, <font color=\"red\">Hidden Markov Models (HMM)</font> that \n",
    "    - capture “<font color=\"red\">story flow</font>”—\n",
    "        - what topics are discussed in what order in the domain— can be trained. \n",
    "* These models \n",
    "    - capitalize \n",
    "        - on the fact that \n",
    "            - within a specific domain, \n",
    "                - information in different texts \n",
    "                - is presented following a common presentation flow. \n",
    "    - For example, \n",
    "        - <font color=\"blue\">news articles about earthquakes</font> often \n",
    "            - <font color=\"blue\">first talk about where</font> the earthquake happened, \n",
    "            - <font color=\"blue\">what its magnitude</font> was, \n",
    "            - then mention <font color=\"blue\">human casualties or damage</font>, \n",
    "            - and finally discuss <font color=\"blue\">rescue efforts</font>. \n",
    "    - Such “story flow” can be learned \n",
    "        - from multiple articles from the same domain. \n",
    "* <font color=\"red\">States in the HMM</font> \n",
    "    - correspond to <font color=\"red\">topics</font> in the domain, \n",
    "    - which are discovered via iterative clustering of similar sentences from many articles from the domain of interest. \n",
    "    - Each state (topic) is \n",
    "        - <font color=\"red\">characterized by a probability distribution</font> \n",
    "            - which indicates how likely a given word is to appear \n",
    "            - in a sentence that discusses the topic. \n",
    "    - <font color=\"blue\">Transitions between states</font> in the model \n",
    "        - correspond to <font color=\"blue\">topic transitions</font> in typical texts. \n",
    "* These HMM models \n",
    "    - do not require any labelled data for training \n",
    "    - and allow for both \n",
    "        - content selection and \n",
    "        - ordering in summarization. \n",
    "    - The sentences that have highest probability of conveying important topics are selected in the summary.\n",
    "    \n",
    "* Even simpler approach \n",
    "    - to discovering the topics \n",
    "    - in a specific domain \n",
    "    - can be applied \n",
    "        - when there are available samples from the domain \n",
    "            - that are more structured \n",
    "            - and contain human-written headings. \n",
    "    - For example, \n",
    "        - there are plenty of Wikipedia articles \n",
    "            - about actors \n",
    "            - and diseases. \n",
    "        - Clustering similar section headings, \n",
    "            - where similarity is defined by cosine similarity for example, \n",
    "                - will identify the topics discussed in each type of article. \n",
    "        - The clusters \n",
    "            - with most headings \n",
    "            - represent \n",
    "                - the most common topics, and \n",
    "                    - the most common string in the cluster is used to label it. \n",
    "        - This procedure \n",
    "            - discovers \n",
    "                - for example that \n",
    "                    - when talking about actors, \n",
    "                    - writers most often include information \n",
    "                        - about their biography, \n",
    "                        - early life, \n",
    "                        - career and \n",
    "                        - personal life. \n",
    "        - Then to summarize web pages \n",
    "            - returned by a search for a specific actor, \n",
    "                - the system can create a Wikipedia-like web page on the fly, \n",
    "                - selecting sentences from the returned results that convey these topics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Influence of Context "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 3.1 Web Summarization \n",
    "* 3.2 Summarization of Scientific Articles\n",
    "* 3.3 Query-focused Summarization\n",
    "* 3.4 Email Summarization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Web Summarization "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* web page context\n",
    "* pages\n",
    "* link\n",
    "* hyperlink tag pointing to the page\n",
    "* This text often provides a descriptive summary of a web page (e.g., “Access to papers published within the last year by members of the NLP group”). \n",
    "* multimedia\n",
    "* it may be hard for a summarizer to distinguish good summary content from bad\n",
    "* a sentence that is a reference to the page (e.g., “CNN is a news site”) as opposed to content (e.g., “The top story for today...”). \n",
    "* In summarization of blog posts\n",
    "    - important sentences are identified based on word frequency\n",
    "    - frequency is computed over the comments on the post rather then the original blog entry"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Summarization of Scientific Articles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Impact summarization\n",
    "    - task of extracting sentences from a paper that represent the most influential content of that paper\n",
    "* Language models\n",
    "    - is built using the collection of all reference areas to a paper,\n",
    "    - giving the probability of each word to occur in a reference area\n",
    "* important sentences are those that convey information similar to that which later papers discussed when referring to the original paper\n",
    "* KL divergence\n",
    "* The final score of a sentence is a linear combination of \n",
    "    - impact importance coming from KL divergence \n",
    "    - intrinsic importance coming from the word probabilities in the input article"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 Query-focused Summarization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* importance of each sentence will be determined by a combination of two factors\n",
    "    - how relevant is that sentence to the user question\n",
    "    - how important is the sentence in the context of the input in which it appears. \n",
    "* There are two classes of approaches to this problem. \n",
    "    - The first adapts techniques for generic summarization of news\n",
    "        - an approach using topic signature words\n",
    "        - extended for query-focused summarization\n",
    "        - a word has probability zero of appearing in a summary for a user defined topic if it neither appears in the \n",
    "            - user query nor is \n",
    "            - a topic signature word for the input\n",
    "    - Other approaches have been \n",
    "        - developed that use new methods for identifying \n",
    "            - relevant and \n",
    "            - salient sentences. \n",
    "        - for specific types of queries\n",
    "        - For example, \n",
    "            - many people have worked on generation of biographical summaries, \n",
    "                - where the query is the name of the person for whom a biography should be generated. \n",
    "                - Most people use some balance of top-down driven approaches that search for patterns of information that might be found in a biography"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.4 Email Summarization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* unique characteristics of email\n",
    "* a distinct linguistic genre\n",
    "    - exhibits characteristics of both \n",
    "        - written text and \n",
    "        - spoken conversation\n",
    "* A thread or a mailbox\n",
    "* conversations \n",
    "* between two or more participants \n",
    "* over time\n",
    "* Unlike spoken dialog, however, the summarizer need not concern itself with speech recognition errors, the impact of pronunciation, or the availability of speech features such as prosody"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Indicator Representations and Machine Learning for Summarization "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 4.1 Graph Methods for Sentence Importance \n",
    "* 4.2 Machine Learning for Summarization "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Indicator representation approaches do not attempt to interpret or represent the topics discussed in the input. Instead they come up with a representation of the text that can be used to directly rank sentences by importance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1 Graph Methods for Sentence Importance "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* In the graph models \n",
    "    - inspired by the PageRank algorithm, \n",
    "    - the input is represented as a highly connected graph. \n",
    "    - Vertices represent sentences and \n",
    "    - edges between sentences are assigned weights equal to the similarity between the two sentences. \n",
    "* The method most often used to compute similarity is cosine similarity with TF*IDF weights for words\n",
    "* Sometimes, instead of assigning weights to edges, the connections be- tween vertices can be determined in a binary fashion.\n",
    "* Graph-based approaches have been shown to \n",
    "    - work well for both \n",
    "        - single- document and \n",
    "        - multi-document summarization\n",
    "* At the same time, in- corporating syntactic and semantic role information in the building of the text graph leads to superior results over plain TF*IDF cosine similarity.\n",
    "* Using different weighting schemes for links between sentences that belong to the same article and sentences from different articles can help separate the notions of topicality within a document and recurrent topics across documents."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 Machine Learning for Summarization "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Selecting Summary Sentences "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 5.1 Greedy Approaches: Maximal Marginal Relevance \n",
    "* 5.2 Global Summary Selection "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most summarization approaches choose content sentence by sentence: they first include the most informative sentence, and then if space constraints permit, the next most informative sentence is included in the summary and so on. Some process of checking for similarity between the chosen sentences is also usually employed in order to avoid the inclusion of repetitive sentences."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.1 Greedy Approaches: Maximal Marginal Relevance "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Maximal Marginal Relevance (MMR)\n",
    "* In this approach, summaries are created using greedy, sentence- by-sentence selection. At each selection step, the greedy algorithm is constrained to select the sentence that is maximally relevant to the user query (or has highest importance score when a query is not available) and minimally redundant with sentences already included in the summary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.2 Global Summary Selection "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Global optimization algorithms can be used to solve the new formulation of the summarization task, in which the best overall summary is selected. * Given some constraints imposed on the summary, such as maximizing informativeness, minimizing repetition, and conforming to required summary length, the task would be to select the best summary. \n",
    "* Finding an exact solution to this problem is NP-hard, but approximate solutions can be found using a dynamic programming algorithm\n",
    "* Exact solutions can be found quickly via search techniques when the sentence scoring function is local, computable only from the given sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 참고자료"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
