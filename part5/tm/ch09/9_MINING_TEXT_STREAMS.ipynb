{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Chapter 9 MINING TEXT STREAMS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Contents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "1. Introduction\n",
    "2. Clustering Text Streams\n",
    "    - 2.1 Topic Detection and Tracking in Text Streams\n",
    "3. Classification of Text Streams\n",
    "4. Evolution Analysis in Text Streams\n",
    "5. Conclusions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The large amount of text data which are continuously produced over time in a variety of large scale applications such as social networks results in massive streams of data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Typically massive text streams are created by very large scale interactions of individuals, or by structured creations of particular kinds of content by dedicated organizations. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An example in the latter category would be the massive text streams cre- ated by news-wire services. Such text streams provide unprecedented challenges to data mining algorithms from an efficiency perspective. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this chapter, we review text stream mining algorithms for a wide variety of problems in data mining such as clustering, classification and topic modeling. We also discuss a number of future challenges in this area of research."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Keywrords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Text Mining, Data Streams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 참고\n",
    "* (2) Text and text stream mining tutorial - http://www.slideshare.net/mgrcar/text-and-text-stream-mining-tutorial-15137759"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# 1. Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some specific examples of applications which create text streams are as follows:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### social networks, news aggregater services, web crawlers\n",
    "In social networks, users continuously communicate with one an- other with the use of text messages.\n",
    "Many news aggregator services 1 may receive large volumes of news articles continuously over time.\n",
    "Many web crawlers may collect a large volume of documents from networks in a small time frame. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### massive volume, summarization, unstructured nature\n",
    "In the case of multi-dimensional and time-series data, such summarization often takes the form of methods such as histograms, wavelets, and sketches which can be used in order to create a structured summary of the underlying data [1]. However, the unstructured nature of text makes the use of such summaries quite challenging."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### This chapter is organized as follows. \n",
    "* In section 2, we will present a variety of the well known algorithms for clustering text streams. \n",
    "    - This includes popular methods for topic detection and tracking in text stream. \n",
    "    - This is because the process of event detection is closely related to the clustering problem. \n",
    "* The methods for classification of text streams are reviewed in section 3. \n",
    "* Section 4 presents methods for evolution analysis of text stream. \n",
    "* The conclusions and summary are presented in section 5."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# 2. Clustering Text Streams\n",
    "* 2.1 Topic Detection and Tracking in Text Streams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### types"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* clustering + machine learning approach\n",
    "    - COBWEB\n",
    "    - CLASSIT\n",
    "    - Many of these methods are extensions of the k-means method as extended to the stream scenario.\n",
    "    - Online Spherical k-Means Algorithm (OSKM)\n",
    "        - This techniques <font color=\"red\">divides up the incoming stream into small segments</font>, each of which can be processed effectively in main memory.\n",
    "        - A <font color=\"red\">decay factor</font> is introduced in order to age-out the old documents, so that the new documents are considered more important from a clustering perspective.\n",
    "    - A distributional modeling method for hierarchical clustering of streaming documents has been proposed in [37].\n",
    "        - <font color=\"red\">hierarchical clustering</font> algorithms\n",
    "        - The method extends the COBWEB and CLASSIT algorithms [20, 25] to the case of text data different method for clustering massive text and categorical data streams is discussed in [3]. \n",
    "        - The approach essentially <font color=\"red\">changes the distributional assumption</font> so that the method can work effectively for text data.\n",
    "    - A different method for clustering massive text and categorical data streams is discussed in [3].\n",
    "        - The method discussed in [3] uses an approach which examines the <font color=\"red\">relationship between outliers, emerging trends, and clusters in the underlying data</font>.\n",
    "        - Old clusters may become inactive, and eventually get replaced by new clusters.\n",
    "        - Similarly, when newly arriving data points do not naturally fit in any particular cluster, these need to be initially classified as outliers. \n",
    "        - However, as time progresses, these new points may create a distinctive pattern of activity which can be recognized as a new cluster.\n",
    "        - The <font color=\"red\">temporal locality</font> of the data stream is manifested by these new clusters.\n",
    "        - For example, the first web page belonging to a particular category in a crawl may be recognized as an outlier, but may later form a cluster of documents of its own. \n",
    "            - On the other hand, the new outliers may not necessarily result in the formation of new clusters. \n",
    "            - Such outliers are true short-term abnormalities in the data since they do not result in the emergence of sustainable patterns. \n",
    "            - The approach discussed in [3] recognizes new clusters by first recognizing them as outliers.\n",
    "\n",
    "* clustering + summrization \n",
    "    - <font color=\"red\">micro-clusters</font>\n",
    "        - This approach works with the use of a summarization methodology, which is motivated by the micro-clustering approach proposed in [4].\n",
    "        - While the concept of micro-clustering was designed for numerical data, it can also be extended to the case of text and categorical data streams.\n",
    "        - condensed droplets\n",
    "            - The concept of micro-clusters is generalized to that of <font color=\"red\">condensed droplets</font> in [3]."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### time-dependent weight & fading function & half-life"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* <font color=\"red\">In order to ensure greater importance of more recent data, a time-sensitive weightage is assigned to each data point.</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### fading function\n",
    "* 참고 : (3) monotonic functions - https://en.wikipedia.org/wiki/Monotonic_function\n",
    "* It is assumed that each data point has a time-dependent weight defined by the function f(t). \n",
    "* The function f(t) is also referred to as the fading function.\n",
    "* The fading function f(t) is a non-monotonic decreasing function which decays uniformly with time t.\n",
    "* In order to formalize this concept, we will define the half-life of a point in the data stream."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### half-life"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Conceptually, the aim of defining a half life is to quantify the rate of decay of the importance of each data point in the stream clustering process"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/cap9.1.png\" width=600 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### decay rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* The decay-rate is defined as the inverse of the half life of the data stream."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* We denote the decay rate by "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/cap9.2.png\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* We denote the weight function of each point in the data stream by "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/cap9.3.png\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* From the perspective of the clustering process, the weight of each data point is f(t). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* It is easy to see that this decay function creates a half life of 1/λ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* It is also evident that by changing the value of λ, it is possible to change the rate at which the importance of the historical information in the data stream decays. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* The higher the value of λ, the lower the importance of the historical information compared to more recent data.\n",
    "    - For more stable data streams, it is desirable to pick a smaller value of λ, whereas for rapidly evolving data streams, it is desirable to pick a larger value of λ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### cluster death"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* When a cluster is created during the streaming process by a newly arriving data point, it is allowed to remain as a trend-setting outlier for at least one half-life. \n",
    "* During that period, if at least one more data point arrives, then the cluster becomes an active and mature cluster. \n",
    "* On the other hand, if no new points arrive during a half-life, then the trend-setting outlier is recognized as a true anomaly in the data stream. At this point, this anomaly is removed from the list of current clusters. \n",
    "* We refer to the process of removal as cluster death."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### droplet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* we describe the process of creation of a condensed-droplet from the underlying text stream. \n",
    "* <font color=\"red\">An important point to remember is that a text data set can be treated as a sparse numeric data set</font>. \n",
    "* This is because most documents contain only a small fraction of the vocabulary with non-zero frequency."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* For a cluster of documents C at time t, we denote the corresponding condensed droplet by D(t,C)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/cap9.4.png\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/cap9.5.png\" width=600 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/cap9.6.png\" width=600 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/cap9.7.png\" width=600 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cluster droplet for the union of two clusters is the sum of individ- ual entries. The only exception is the last entry which is the maxima of the two last-update times. We note that the additivity property provides considerable convenience for data stream processing since the entries can be updated efficiently using simple additive operations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/cap9.8.png\" width=600 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The second observation is critical in regulating the rate at which the clus- ter droplets are updated during the clustering process. Since all cluster droplets decay at essentially the same rate (unless new data points are added), it follows that it is not necessary to update the decay statistics at each time stamp. Rather, each cluster droplet can be updated lazily, whenever new data points are added to it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### The overall algorithm proceeds as follows. \n",
    "* At the beginning of algorithmic execution, we start with an empty set of clusters. \n",
    "* As new data points arrive, unit clusters containing individual data points are created. \n",
    "* Once a maximum number k of such clusters have been created, we can begin the process of online cluster maintenance. \n",
    "* Thus, we initially start off with a trivial set of k clusters. \n",
    "* These clusters are updated over time with the arrival of new data points.\n",
    "* When a new data point X arrives, its similarity to each cluster droplet is computed. (In the case of text data sets, the cosine similarity measure)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/cap9.9.png\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### A different way of utilizing the temporal evolution of text documents & bursty features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A different way of utilizing the temporal evolution of text documents in the clustering process is described in [26]."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### bursty features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Specifically, the work in [26] uses bursty features as markers of new topic occurrences in the data stream. This is because the semantics of an up-and-coming topic are often reflected in the frequent presence of a few distinctive words in the text stream. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* A specific example illustrates the bursty features in two topics corresponding to the two topics of “US Mid-Term Elections” and “Newt Gingrich resigns from House” respectively. The corresponding bursty features [26] which occurred frequently in the newsstream during the period for these topics were as follows:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/cap9.10.png\" width=600 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"red\">It is evident that at a given period in time, the nature of relevant topics could lead to bursts in specific features of the data stream.</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### bursty feature representation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* There- fore, the method discussed in [26] uses a new representation, which is re- ferred to as the bursty feature representation for mining text streams. In this representation, a time-varying weight is associated with the features depending upon its burstiness. This also reflects the varying importance of the feature to the clustering process. Thus, it is important to remem- ber that a particular document representation is dependent upon the particular instant in time at which it is constructed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### implicit reduction in dimensionality"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another issue which is handled effectively in this approach is an im- plicit reduction in dimensionality of the underlying collection. Text is inherently a high dimensional data domain, and the pre-selection of some of the features on the basis of their burstiness can be a natural way to reduce the dimensionality of document representation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### feature extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"red\">In general, it is evident that feature extraction is important for all clustering algorithms.</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* phrase extraction\n",
    "     - topic signatures = The phrases which are extracted from the collection are also referred to as topic signatures.\n",
    "* semantic smoothing\n",
    "    - The use of such phrasal clarification for improving the quality of the clustering is referred to as semantic smoothing because it reduces the noise which is associated with semantic ambiguity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### topic-signature model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* For each document, it is assumed that all terms in it are generated either by a topic-signature model, or a background collection model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* The approach in [32] works by modeling the soft probability p(w|C_j) for word w and cluster C_j .\n",
    "    - we use the frequency f(w,d) of word w in document\n",
    "d. 􏰂"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/cap9.11.png\" width=600 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### cluster profile\n",
    "* The work in [32] maintains these probabilities in online fashion with the use of a cluster profile, that weights the probabilities with the use of a fading function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* OCTS\n",
    "* OCTSM\n",
    "* HITS-like spectral technique\n",
    "* topic modeling\n",
    "* SparseLDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 2.1 Topic Detection and Tracking in Text Streams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* A closely related problem to clustering is that of topic detection and tracking [5, 11, 24, 46, 47, 51]. \n",
    "* In this problem, we create unsupervised clusters from a text stream, and then determine the sets of clusters which match real events.\n",
    "* These real events may correspond to documents which are identified by a human.\n",
    "* Since the problem of <font color=\"red\">online topic detection</font> is closely related to that of clustering, we will discuss this problem as a subsection of our broader discussion on clustering, though not all methods for topic detection use clustering techniques."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/cap9.12.png\" width=600 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/cap9.13.png\" width=600 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One general observation about the online topic detection and track- ing problem [6, 48] is that this problem is quite hard in general, and the performance of <font color=\"red\">first event detection can degrade under a variety of scenarios</font>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to improve the effectiveness of first event detection, the work in [48] proposes to use the training data of old events in order to learn useful statistics for the prediction of new events. The broad approach in [48] contains the following components:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* The documents are classified into broad topics, each of which consists of multiple events.\n",
    "* Named entities are identified, optimizing their weight relative to normal words for each topic, and computing a stopword list for each topic.\n",
    "* Measuring the novelty of a new document conditioned on the system-predicted topic for that document."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### source-specific tf–idf model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* For example, a source-specific tf–idf model is maintained in which the statistics are specific to each news source. Similarly, the approach normalizes for the fact that documents which come from the same source tend to have a higher similarity because of the vocabulary conventions which are often used in the similarity computation.\n",
    "* A method for online topic detection and tracking is presented in [51] as an application of stream clustering. This work uses a probabilistic LDA model in order to create an online model for estimating the growing number of clusters. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### feature-pivot clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* parameter-free bursty event detection\n",
    "* Another method for fast and parameter-free bursty event detection is proposed in [24]. \n",
    "* This approach focusses on finding bursty features which characterize the presence of an event. \n",
    "* In order to achieve this goal, the technique in [24] proposes a feature-pivot clustering, which groups features on the basis of bursty co-occurrence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### keyword graphs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* The problem of event detection has also been studied with the use of keyword graphs in [39]. \n",
    "* The work in [39] builds a keywords graph from a text stream in which a node corresponds to a keyword, and an edge is added to the keyword graphs when a pair of words occur together in the document. \n",
    "* Events are characterized as communities of keywords in this network. This broad approach is used in the context of a window-based technique for the case of social streams.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### social dimensions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* In the context of social network streams, a natural question arises, as to whether one can use any of the social dimensions of the underlying stream in order to improve the underlying event detection process. \n",
    "* Such an approach has been proposed in [53], in which events are determined by combining text clustering, social network structure clustering, and temporal segmentation.\n",
    "* In this case, an event is defined as a set of text documents which are semantically coherent, and are structurally well connected both in terms of social network structure and time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* These three different characteristics are used in the following ways:\n",
    "    - First, the content is used in order to create a hierarchy of topics from the social text stream.\n",
    "    - While the topical patterns are useful for distinguishing content, the temporal segmentation is used in order to distinguish different events. The assumption is that the communication between different parties happen during a short contiguous time period.\n",
    "    - Since it is assumed that the events occur between connected entities, we use the connectivity between the different events in order to further segment the events. An edge between a pair of nodes corresponds to a communication between the social entities. Multiple edges are allowed between pairs of nodes. This structure is used in order to determine the event-dependent communities."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LSH"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Another method [36] has been proposed in the context of the Twitter social network data set.\n",
    "* In this paper, a locality sensitive hashing method (LSH) is used in order to keep track of the different documents.\n",
    "    - The idea in LSH [14] is to use a hashing scheme in which the probabil- ity of hashing two documents into the same bucket is proportional to the cosine of the angle between the two documents.\n",
    "* In the event that the corresponding distance is above a given threshold, we can declare the underlying story as novel."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### multiple text streams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Most of the work in text stream mining and topic detection is performed in the context of a single news stream. \n",
    "* In many cases, we have multiple text streams [44], which are indexed by the same set of time points (called coordinated text streams). \n",
    "    - For example, when a major event happens, all the news articles published by different agencies in different languages cover the same event in that period.\n",
    "    - This is referred to as a <font color=\"red\">correlated bursty topic pattern</font> in the different news article streams."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# 3. Classification of Text Streams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* With the use of the condensed droplet data structure, it is possible to extend all the methods discussed in [2] to the text stream scenario.\n",
    "* The problem of text stream classification arises in the context of a number of different IR tasks.\n",
    "    - news filtering\n",
    "        - In this case, the training data may be available for batch learning, but the test data may arrive in the form of a stream.\n",
    "        - The scenario is usually easy to handle, because most classifier models are compact and classify individual test instances efficiently. \n",
    "    - email spam filtering \n",
    "        - In this case, both the training and the test data may arrive in the form of a stream. The patterns in the training data may continuously change over time, as a result of which the models need to be updated dynamically.\n",
    "        - in the scenario, the training model needs to be constantly updated in order to account for changes in the patterns of the underlying training data.\n",
    "* Specifically, the approach has been applied to the Naive Bayes, Rocchio, and k-nearest neighbor classification algorithms. It has been shown that the incorporation of temporal weighting factors is useful in improving the classification accuracy, when the underlying data is evolving over time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# 4. Evolution Analysis in Text Streams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* A key problem in the case of text is to determine evolutionary patterns in temporal text streams. An early survey on the topic of evolution anal- ysis in text streams may be found in [28]. Such evolutionary patterns can be very useful in a wide variety of applications, such as summariz- ing events in news articles and revealing research trends in the scientific literature.\n",
    "    - For example, an event may have a life cycle in the underlying theme patterns such as the beginning, duration, and end of a particular event.\n",
    "* Similarly, the evolution of a particular topic in the research literature may have a life-cycle in terms of how the different topics af- fect one another. \n",
    "    - This problem was defined in [33], and contains three main parts: \n",
    "        - (a) Discovering the themes in text; \n",
    "        - (b) creating an evolution graph of themes; and \n",
    "        - (c) studying the life cycle of themes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### theme"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A theme is defined as a semantically related set of words, with a corresponding probability distribution, which coherently represents a particular topic or sub-topic."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* This corresponds to a model, which is represented by θ. \n",
    "* The span of such a theme represents the starting and end point of the corresponding theme in terms of the time-interval"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/cap9.16.png\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Thus, the theme span is denoted by the triple"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/cap9.15.png\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"red\">As time passes, a particular theme γ1 may perform a transition into another theme γ2. A theme γ1 is said to have evolved into another theme γ2, if there is sufficient similarity between their corresponding spans.</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* theme evolution graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/cap9.17.png\" width=600 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The overall approach requires three steps:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* In the first step, we segment the text stream into a number of possibly overlapping sub-collections with fixed or variable time spans. This is done in an application-specific way.\n",
    "* The salient themes are determined from each collection with the use of a probabilistic mixture model. A standard mixture model technique [19] was used for this purpose.\n",
    "* Finally, all the evolution transitions are determined from these theme patterns. This is done with the use of the KL-divergence measure in order to compute the evolution distance between two themes. In the event that the similarity is above a given threshold, the transition is considered valid."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In addition, a method is proposed in [33] for analyzing the entire theme life cycle by measuring the strength of the theme over different periods. A method based on HMM is proposed to measure the theme-shifts over the entire period as well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The problem of tracking new topics, ideas, and memes across the Web\n",
    "* The problem of tracking new topics, ideas, and memes across the Web has been studied in [29]. \n",
    "* This problem is related to that of the topic detection and tracking discussed earlier. \n",
    "* However, the rate of evolution in the web and blog scenario is significantly greater than the models which have been discussed in earlier work."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# 5. Conclusions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This chapter studies the problem of mining text streams. The challenge in the case of text stream arises because of its temporal and dynamic nature in which the patterns and trends of the stream may vary rapidly over time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 참고자료"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* (1) Mining Text Data - http://link.springer.com/book/10.1007/978-1-4614-3223-4/page/1\n",
    "* (2) Text and text stream mining tutorial - http://www.slideshare.net/mgrcar/text-and-text-stream-mining-tutorial-15137759\n",
    "(3) monotonic functions - https://en.wikipedia.org/wiki/Monotonic_function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
