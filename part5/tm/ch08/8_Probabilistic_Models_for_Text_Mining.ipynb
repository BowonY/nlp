{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8. Probabilistic Models for Text Mining"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 바벨피쉬 / 바벨피쉬Py : 파트 5 - 텍스트마이닝 [1]\n",
    "* 김무성"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Contents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Introduction \n",
    "2. Mixture Models \n",
    "3. Stochastic Processes in Bayesian Nonparametric Models \n",
    "4. Graphical Models \n",
    "5. Probabilistic Models with Constraints \n",
    "6. Parallel Learning Algorithms \n",
    "7. Conclusions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Introduction "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The major probabilistic models covered in this chapter include:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Mixture Models\n",
    "    - PLSA\n",
    "    - LDA\n",
    "* Bayesian Nonparametric Models\n",
    "    - Dirichlet process\n",
    "* Bayesian Networks\n",
    "* Hidden Markov Model\n",
    "    - part-of-speech tagging in NLP\n",
    "* Markov Random Fields\n",
    "* Conditional Random Fields\n",
    "    - Name entity recognition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Mixture Models "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 2.1 General Mixture Model Framework \n",
    "* 2.2 Variations and Applications \n",
    "* 2.3 The Learning Algorithms "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://www.vtkjournal.org/download/logopublication/4876/big\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 General Mixture Model Framework "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://cfile8.uf.tistory.com/image/257DDF35514C692E232BE5\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/cap8.1.png\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/cap8.2.png\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/cap8.3.png\" width=600 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From generative process point of view, each observed data xi is generated by:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/cap8.4.png\" width=600 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example: Mixture of Unigrams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://yosinski.com/mlss12/media/slides/MLSS-2012-Domingos-Statistical-Relational-Learning_082.png\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://reference.wolfram.com/language/ref/Files/MultinomialDistribution.en/O_1.gif\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://openeco.eu/images/home/text-mining/bag-of-words.png\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://www.xperseverance.net/blogs/wp-content/uploads/image/LDA_mixture_of_unigram.png\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A document di composed of a bag of words wi = (ci,1, ci,2, . . . , ci,m), where m is the size of the vocabulary and ci,j is the number of term wj in document di, is considered as a mixture of unigram language models. That is, each component is a multinomial distribution over terms, with parameters βk,j, denoting the probability of term wj in cluster k, i.e., p(wj|βk), for k = 1,...,K and j = 1,...,m."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The joint probability of observing the whole document collection is then:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/cap8.5.png\" width=600 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* where πk is the proportion weight for cluster k"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Variations and Applications "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 2.2.1 Topic Models\n",
    "* 2.2.2 Other Applications"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.1 Topic Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### PLSA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Probabilistic latent semantic analysis (PLSA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://parkcu.com/blog/wp-content/uploads/2013/06/word-topic-example.png\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/e/ec/Plsi_1.svg/300px-Plsi_1.svg.png\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The probability of observation term wj in di is then defined by the mixture in the following:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/cap8.6.png\" width=600 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* where p(k|di) = p(zi,j = k) is the mixing proportion of different topics for di, βk is the parameter set for multinomial distribution over terms for topic k, and p(wj|βk) = βk,j. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The joint probability of observing all the terms in document di is:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/cap8.7.png\" width=600 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* where wi is the same defined as in the mixture of unigrams and p(di) is the probability of generating di."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* LDA. Latent Dirichlet allocation (LDA) extends PLSA by further adding priors to the parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://yosinski.com/mlss12/media/slides/MLSS-2012-Blei-Probabilistic-Topic-Models_020.png\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://salsahpc.indiana.edu/b649proj/images/proj3_LDA%20structure.png\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://image.slidesharecdn.com/nonparametricbayes-130329120556-phpapp01/95/bayesian-nonparametrics-models-based-on-the-dirichlet-process-28-638.jpg?cb=1364559167\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The probability of observing all the terms in document di is then:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/cap8.8.png\" width=600 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/cap8.9.png\" width=600 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.2 Other Applications"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we briefly introduce some other applications of mixture models in text mining."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Comparative text mining (CTM)\n",
    "    - Given a set of comparable text collections (e.g., the reviews for different brands of laptops), the task of compara- tive text mining is to discover any latent common themes across all collections as well as special themes within one collection."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://i0.wp.com/statistical-research.com/wp-content/uploads/2012/10/Rplot.png\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Contextual text mining (CtxTM)\n",
    "    - which extracts topic models from a collection of text with context information (e.g., time and location) and models the variations of topics over different context."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://i.ytimg.com/vi/7KLSo9d4Xzg/hqdefault.jpg\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://i.ytimg.com/vi/t1LBEjtC6gc/hqdefault.jpg\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://i.ytimg.com/vi/esGqS120klg/mqdefault.jpg\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Topic Sentiment Mixture (TSM)\n",
    "    - which aims at modeling facets and opinions in we-blogs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://www2007.org/htmlpapers/paper680/fp680-mei-img3.png\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://image.slidesharecdn.com/opinion-analysis1-150123152807-conversion-gate02/95/statistical-methods-for-integration-and-analysis-of-online-opinionated-text-data-30-638.jpg?cb=1422028206\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://www2007.org/htmlpapers/paper680/fp680-mei-img30.png\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 The Learning Algorithms "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 2.3.1 Overview\n",
    "* 2.3.2 EM Algorithm\n",
    "* 2.3.3 Gibbs Sampling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3.1 Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The general idea of learning parameters in mixture models (and other probabilistic models) is to find a set of “good” parameters θ that maximizes the probability of generating the observed data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Two estimation criterions are frequently used,"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* maximum-likelihood estimation (MLE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://www.ncbi.nlm.nih.gov/Class/NAWBIS/Modules/Phylogenetics/images/phylonv48.gif\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://doc.openturns.org/openturns-0.13.2/doc/html/ReferenceGuide/output/OpenTURNS_ReferenceGuide197x.png\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://statgen.iop.kcl.ac.uk/media/ml1.gif\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* maximum-a-posteriori-probability (MAP)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://www.lancaster.ac.uk/pg/jamest/Group/images/bayesthm.JPG\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://i1.wp.com/statistical-research.com/wp-content/uploads/2013/09/prior-likelihood-posterior1.png\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://images.slideplayer.com/17/5277039/slides/slide_4.jpg\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The likelihood (or likelihood function) of a set of parameters given the observed data is defined as the probability of all the observations under those parameter values. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/cap8.10.png\" width=600 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most of the time, log-likelihood is optimized instead, as it converts products into summations and makes the computation easier:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/cap8.11.png\" width=600 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When priors are incorporated to the mixture models (such as in LDA), the MAP estimation is used instead, which is to find a set of parameters θ that maximizes the posterior density function of θ given the observed data:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/cap8.12.png\" width=600 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3.2 EM Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://cse-wiki.unl.edu/wiki/images/thumb/a/a3/EM.jpg/400px-EM.jpg\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://i.stack.imgur.com/mj0nb.gif\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://upload.wikimedia.org/wikipedia/commons/6/69/EM_Clustering_of_Old_Faithful_data.gif\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For mixture models, the likelihood function can be further viewed as the marginal over the complete likelihood involving hidden variables:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/cap8.13.png\" width=600 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The log-likelihood function is then:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/cap8.14.png\" width=600 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### E-step (Expectation step)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/cap8.15.png\" width=600 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### M-step (Maximization-step)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/cap8.16.png\" width=600 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are several variants for EM algorithm when the original EM algorithm is difficult to compute, and some of which are listed in the following:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Generalized EM. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Variational EM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://parkcu.com/blog/wp-content/uploads/2013/07/variational-inference.png\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3.3 Gibbs Sampling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Markov chain Monte Carlo (MCMC)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://parkcu.com/blog/wp-content/uploads/2013/08/monte_carlo_integration.gif\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://cs.brown.edu/courses/cs242/lectures/images/mcmc.png\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://fedc.wiwi.hu-berlin.de/xplore/ebooks/html/csa/img1321.gif\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The hidden cluster zi,j for term wi,j, i.e., the term wj in document di, is sampled according to the conditional distribution of zi,j, given the observations of all the terms as well as the their hidden cluster labels except for wi,j in the corpus:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/cap8.17.png\" width=600 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Stochastic Processes in Bayesian Nonparametric Models "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 3.1 Chinese Restaurant Process \n",
    "* 3.2 Dirichlet Process \n",
    "* 3.3 Pitman-Yor Process \n",
    "* 3.4 Others "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 참고자료\n",
    "* [12] Stat 547Q : Statistical Modeling with Stochastic Processes - http://www.stat.ubc.ca/~bouchard/courses/stat547-sp2011/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bayesian nonparametric models "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://raw.githubusercontent.com/psygrammer/coco/d31b61b25bbf42f869621fe02104df4a9e1c413e/part2/compsy/study04/figures/fig9.1.png\" width=600 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Chinese Restaurant Process "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 참고자료 \n",
    "* [4] Chinese Restaurant Process - http://www.slideshare.net/MohitdeepSingh/chinese-restaurant-process"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://raw.githubusercontent.com/psygrammer/coco/d31b61b25bbf42f869621fe02104df4a9e1c413e/part2/compsy/study04/figures/fig9.2.png\" width=600 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/cap8.18.png\" width=600 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/cap8.19.png\" width=600 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Dirichlet Process "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 3.2.1 Overview of Dirichlet Process\n",
    "* 3.2.2 Dirichlet Process Mixture Model\n",
    "* 3.2.3 The Learning Algorithms\n",
    "* 3.2.4 Applications in Text Mining"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 참고\n",
    "* [5] Digging into the Dirichlet Distribution - http://www.slideshare.net/g33ktalk/machine-learning-meetup-12182013\n",
    "* [6] Bayesian Nonparametrics: Models Based on the Dirichlet Proces - http://www.slideshare.net/AlessandroPanella1/nonparametric-bayes\n",
    "* [7] Bayesian Nonparametric Topic Modeling Hierarchical Dirichlet Processes - http://www.slideshare.net/NoSyu/bayesian-nonparametric-topic-modeling-hierarchical-dirichlet-processes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2.1 Overview of Dirichlet Process"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/cap8.20.png\" width=600 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/cap8.21.png\" width=600 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/cap8.22.png\" width=600 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2.2 Dirichlet Process Mixture Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/cap8.23.png\"  />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/cap8.24.png\" width=600 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2.3 The Learning Algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/cap8.25.png\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2.4 Applications in Text Mining"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 Pitman-Yor Process "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 참고\n",
    "* [8] A Hierarchical Nonparametric Bayesian Approach to Statistical Language Model Domain Adaptation - https://docs.google.com/viewer?url=http%3A%2F%2Fece.duke.edu%2F~lcarin%2FMingyuan12.18.09.ppt\n",
    "* [9] Hierarchical Bayesian Models of Language and Text - http://www.stats.ox.ac.uk/~teh/research/compling/bayeslm.pdf\n",
    "* [10] Dirichlet Process and Stick-Breaking - http://web.cse.ohio-state.edu/~kulis/teaching/788_sp12/scribe_notes/lecture14.pdf\n",
    "* [11] Lecture 10: More on hierarchical models and PY. Infinite HMM, Beta process - http://www.stat.ubc.ca/~bouchard/courses/stat547-sp2011/lecture10.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/cap8.26.png\" width=600 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.4 Others "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Graphical Models "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 4.1 Bayesian Networks \n",
    "* 4.2 Hidden Markov Models \n",
    "* 4.3 Markov Random Fields \n",
    "* 4.4 Conditional Random Fields \n",
    "* 4.5 Other Models "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 참고\n",
    "* [13] Probabilistic Graphical Models - https://www.coursera.org/course/pgm\n",
    "* [14] stanford-pgm/slides/Section-1-Introduction - http://spark-university.s3.amazonaws.com/stanford-pgm/slides/Section-1-Introduction-Combined.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1 Bayesian Networks "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 4.1.1 Overview\n",
    "* 4.1.2 The Learning Algorithms\n",
    "* 4.1.3 Applications in Text Mining"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 참고\n",
    "* [15] Bayesian Network Fundamentals - http://spark-university.s3.amazonaws.com/stanford-pgm/slides/Section-2-Representation-Bayes-Nets.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1.1 Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Conditional Independence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Factorization Definition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/cap8.27.png\" width=600 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/cap8.28.png\" width=600 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/cap8.29.png\" width=600 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1.2 The Learning Algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1.3 Applications in Text Mining"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 Hidden Markov Models "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 4.2.1 Overview\n",
    "* 4.2.2 The Learning Algorithms\n",
    "* 4.2.3 Applications in Text Mining"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 참고\n",
    "* [16] Template Models - http://spark-university.s3.amazonaws.com/stanford-pgm/slides/Section-2-Representation-Template-Models.pdf\n",
    "* [17] Hidden Markov models, graphical models - https://www.cs.berkeley.edu/~jordan/courses/294-fall09/lectures/hmm/slides.ppt\n",
    "* [19] Graphical models and Hidden Markov Models - http://www.asl.ethz.ch/education/master/info-process-rob/graph_HMM.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/cap8.31.png\" width=600 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/cap8.30.png\" width=600 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2.1 Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/cap8.32.png\" width=600 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2.2 The Learning Algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/cap8.33.png\" width=600 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/cap8.34.png\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/cap8.35.png\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Baum-Welch algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2.3 Applications in Text Mining"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.3 Markov Random Fields "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 4.3.1 Overview\n",
    "* 4.3.2 The Learning Algorithms\n",
    "* 4.3.3 Applications in Text Mining"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 참고\n",
    "* [18] Markov Network Fundamentals http://spark-university.s3.amazonaws.com/stanford-pgm/slides/Section-2-Representation-Markov-Nets.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3.1 Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Conditional Independence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Clique Factorization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/cap8.36.png\" width=600 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/cap8.37.png\" width=600 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/cap8.38.png\" width=600 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3.2 The Learning Algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/cap8.40.png\" width=600 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/cap8.41.png\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3.3 Applications in Text Mining"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.4 Conditional Random Fields "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 4.4.1 Overview\n",
    "* 4.4.2 The Learning Algorithms\n",
    "* 4.4.3 Applications in Text Mining"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 참고\n",
    "* [20] Conditional Random Fields and beyond …  - http://web.engr.illinois.edu/~khashab2/files/2013_crf.pptx\n",
    "* [21] An Introduction to Conditional Random Field - http://archer.ee.nctu.edu.tw/powerpoint/CRF_2.pptx\n",
    "* [22] Conditional Random Fields -  http://www.cedar.buffalo.edu/~srihari/CSE574/Chap13/Ch13.5-ConditionalRandomFields.pdf\n",
    "* [23] Conditional Random Fields - Stanford NLP Group - http://nlp.stanford.edu/software/je"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4.1 Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/cap8.42.png\" width=600 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/cap8.39.png\" width=600 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4.2 The Learning Algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4.3 Applications in Text Mining"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.5 Other Models "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Probabilistic Models with Constraints "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Parallel Learning Algorithms "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Conclusions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 참고자료"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* [1] Mining Text Data - http://link.springer.com/book/10.1007/978-1-4614-3223-4/page/1\n",
    "* [2] EM - http://www.cs.tut.fi/kurssit/TLT-5906/EM_presentation_2013.pdf\n",
    "* [3] Variational Inference (LDA) - http://parkcu.com/blog/latent-dirichlet-allocation/\n",
    "* [4] Chinese Restaurant Process - http://www.slideshare.net/MohitdeepSingh/chinese-restaurant-process\n",
    "* [5] Digging into the Dirichlet Distribution - http://www.slideshare.net/g33ktalk/machine-learning-meetup-12182013\n",
    "* [6] Bayesian Nonparametrics: Models Based on the Dirichlet Proces - http://www.slideshare.net/AlessandroPanella1/nonparametric-bayes\n",
    "* [7] Bayesian Nonparametric Topic Modeling Hierarchical Dirichlet Processes - http://www.slideshare.net/NoSyu/bayesian-nonparametric-topic-modeling-hierarchical-dirichlet-processes\n",
    "* [8] A Hierarchical Nonparametric Bayesian Approach to Statistical Language Model Domain Adaptation - https://docs.google.com/viewer?url=http%3A%2F%2Fece.duke.edu%2F~lcarin%2FMingyuan12.18.09.ppt\n",
    "* [9] Hierarchical Bayesian Models of Language and Text - http://www.stats.ox.ac.uk/~teh/research/compling/bayeslm.pdf\n",
    "* [10] Dirichlet Process and Stick-Breaking - http://web.cse.ohio-state.edu/~kulis/teaching/788_sp12/scribe_notes/lecture14.pdf\n",
    "* [11] Lecture 10: More on hierarchical models and PY. Infinite HMM, Beta process - http://www.stat.ubc.ca/~bouchard/courses/stat547-sp2011/lecture10.pdf\n",
    "* [12] Stat 547Q : Statistical Modeling with Stochastic Processes - http://www.stat.ubc.ca/~bouchard/courses/stat547-sp2011/\n",
    "* [13] Probabilistic Graphical Models - https://www.coursera.org/course/pgm\n",
    "* [14] stanford-pgm/slides/Section-1-Introduction - http://spark-university.s3.amazonaws.com/stanford-pgm/slides/Section-1-Introduction-Combined.pdf\n",
    "* [15] Bayesian Network Fundamentals - http://spark-university.s3.amazonaws.com/stanford-pgm/slides/Section-2-Representation-Bayes-Nets.pdf\n",
    "* [16] Template Models - http://spark-university.s3.amazonaws.com/stanford-pgm/slides/Section-2-Representation-Template-Models.pdf\n",
    "* [17] Hidden Markov models, graphical models - https://www.cs.berkeley.edu/~jordan/courses/294-fall09/lectures/hmm/slides.ppt\n",
    "* [18] Markov Network Fundamentals http://spark-university.s3.amazonaws.com/stanford-pgm/slides/Section-2-Representation-Markov-Nets.pdf\n",
    "* [19] Graphical models and Hidden Markov Models - http://www.asl.ethz.ch/education/master/info-process-rob/graph_HMM.pdf\n",
    "* [20] Conditional Random Fields and beyond …  - http://web.engr.illinois.edu/~khashab2/files/2013_crf.pptx\n",
    "* [21] An Introduction to Conditional Random Field - http://archer.ee.nctu.edu.tw/powerpoint/CRF_2.pptx\n",
    "* [22] Conditional Random Fields -  http://www.cedar.buffalo.edu/~srihari/CSE574/Chap13/Ch13.5-ConditionalRandomFields.pdf\n",
    "* [23] Conditional Random Fields - Stanford NLP Group - http://nlp.stanford.edu/software/jenny-ner-2007.ppt\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
